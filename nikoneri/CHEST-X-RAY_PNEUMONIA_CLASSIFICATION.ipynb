{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHEST X-RAY Pneumonia Classification with EfficientNet\n",
    "\n",
    "This notebook captures the **nikoneri** TensorFlow implementation that leverages EfficientNetB3 with staged fine-tuning, cosine decay learning rate schedules, and rich evaluation on the held-out test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from pathlib import Path\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print('GPUs:', tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data pipeline\n",
    "\n",
    "Use `tf.data` pipelines with caching and prefetching to stream the dataset efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = Path('..') / 'data' / 'chest_xray'\n",
    "train_dir = data_root / 'train'\n",
    "val_dir = data_root / 'val'\n",
    "test_dir = data_root / 'test'\n",
    "\n",
    "for split in (train_dir, val_dir, test_dir):\n",
    "    for label in ('NORMAL', 'PNEUMONIA'):\n",
    "        path = split / label\n",
    "        if not path.exists():\n",
    "            raise FileNotFoundError(f'Missing expected folder: {path}')\n",
    "\n",
    "print('Dataset directories verified.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 300\n",
    "BATCH_SIZE = 16\n",
    "SEED = 123\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='binary',\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    val_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='binary',\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='binary',\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "print('Classes:', class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomFlip('horizontal'),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.2),\n",
    "    layers.RandomTranslation(0.1, 0.1)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model definition\n",
    "\n",
    "Initialise EfficientNetB3 with ImageNet weights and append a custom classification head.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = keras.applications.EfficientNetB3(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3)\n",
    ")\n",
    "base_model.trainable = False\n",
    "\n",
    "inputs = keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "x = data_augmentation(inputs)\n",
    "x = keras.applications.efficientnet.preprocess_input(x)\n",
    "x = base_model(x, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dropout(0.4)(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs, name='nikoneri_efficientnet')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compile and initial training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = math.ceil(train_ds.cardinality().numpy())\n",
    "initial_epochs = 10\n",
    "learning_rate = tf.keras.optimizers.schedules.CosineDecay(1e-3, decay_steps=steps_per_epoch * initial_epochs)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history_frozen = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=initial_epochs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Fine-tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:300]:\n",
    "    layer.trainable = False\n",
    "\n",
    "fine_tune_epochs = 15\n",
    "total_epochs = initial_epochs + fine_tune_epochs\n",
    "fine_lr = tf.keras.optimizers.schedules.CosineDecay(1e-4, decay_steps=steps_per_epoch * fine_tune_epochs)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=fine_lr),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history_fine = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=total_epochs,\n",
    "    initial_epoch=history_frozen.epoch[-1] + 1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Learning curves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(histories, metric='accuracy'):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    for label, hist in histories.items():\n",
    "        plt.plot(hist.history[metric], label=f'{label} {metric}')\n",
    "        plt.plot(hist.history[f'val_{metric}'], label=f'{label} val_{metric}')\n",
    "    plt.legend()\n",
    "    plt.title(f'{metric} over epochs')\n",
    "    plt.show()\n",
    "\n",
    "plot_history({'frozen': history_frozen, 'fine': history_fine}, metric='accuracy')\n",
    "plot_history({'frozen': history_frozen, 'fine': history_fine}, metric='loss')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_acc = model.evaluate(val_ds)\n",
    "print(f'Validation accuracy: {val_acc:.4f}')\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_ds)\n",
    "print(f'Test accuracy: {test_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Detailed metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "probabilities = model.predict(test_ds)\n",
    "predictions = (probabilities > 0.5).astype(int)\n",
    "true_labels = np.concatenate([y.numpy() for _, y in test_ds.unbatch().batch(1)])\n",
    "\n",
    "report = classification_report(true_labels, predictions, target_names=class_names)\n",
    "print(report)\n",
    "\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "print('Confusion matrix:\n",
    "', cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('nikoneri_efficientnet_saved_model')\n",
    "model.save('nikoneri_efficientnet.h5')\n",
    "print('Saved EfficientNet model.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}